# 多云监控方案

## 需求描述

1. 构建容错性强的监控与日志系统，确保在成员中心服务异常或下线时，仍能持续采集、存储关键监控指标与日志数据。
2. 数据保存周期短：现有监控数据，无法满足RoCE网络性能回溯与趋势分析需求，但直接延长周期导致存储成本激增,需要测试
3. 采集频率不足：默认15秒采集间隔无法捕捉RoCE网络的瞬时速率波动（如微突发丢包、延迟抖动），影响故障根因定位。
4. 存储与性能瓶颈时序数据库（如Prometheus）在高频写入场景下内存与磁盘IO压力陡增，存在OOM风险。
5. Prometheus 升级验证。

## 方案目标

针对上面的需求描述，总结来说需要从三个方面解决上面的问题：
1. Prometheus升级验证。
2. 提供多云监控监控方案，该方案需要具备如下功能：
   - 更好的兼容 Prometheus、Grafana 等开源工具，减少迁移成本，学习成本。
   - 解决成员集群下线后，部分数据（两个小时，prometheus机制导致）丢失的情况。
   - 方案能提供比Prometheus更加高效的数据存储和压缩能力。在相同的存储空间下保存更长时间的数据。
   - 在高频写入的场景下，相关查询组件能够水平扩展，以支持更高的写入吞吐量，同时确保数据的一致性和可靠性。
3. 提高网络指标采集频率：尽量减少提高采集频率对prometheus的性能产生过大影响。

## Prometheus升级验证

### Prometheus 使用现状 

当前标准部署流程中prometheus的版本是2.41，在长期观察使用过程中，存在以下问题。
1. 在长期存储的场景下，数据压缩和长时间段查询会导致prometheus OOM 并重启 ，导致prometheus的数据存储存在间歇性无法采集到数据的情况。
2. 存在expoter能正常暴露数据，但是prometheus本身无法采集到数据的情况。
3. node-exporter 存在某个节点的数据无法上报的情况。
4. 算法环境存在近期（几天前）的数据无法查询的情况
5. 卡时需求在30天的查询时间段存在Prometheus直接oomkill问题
6. prometheus 配置了很多暂时用不到servicemonitor，采集的数据量很多

![prometheus-1-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-1-2025-03-18.png)
![prometheus-2-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-2-2025-03-18.png)
![prometheus-3-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-3-2025-03-18.png)

### 解决方案

将Prometheus 2.41 版本升级到 Prometheus 3.2.0。

相比较Prometheus 2.41在prometheus 3.2.0版本中，主要的优化点是对`内存`的使用优化。官方描述2.47 版本后prometheus的内存消耗减少了一半[30 Pull Requests Later, Prometheus Memory Use Is Cut in Half - The New Stack](https://thenewstack.io/30-pull-requests-later-prometheus-memory-use-is-cut-in-half/)

参考github版本说明 [Releases · prometheus/prometheus](https://github.com/prometheus/prometheus/releases)

#### Prometheus 3.2.0 版本更新验证

**接口请求时间对比验证**

| 接口名称 |名称| 总请求数 | 失败数 | 失败率 | 平均响应时间(ms) | 最小响应(ms) | 最大响应(ms) | 90% (ms) | 95% (ms) | 99% (ms) | TPS | 平均接收流量每秒请求数 |每秒请求数|
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET http://10.0.101.61:33493/api/v1/query?query=kube_node_info   | prometheus-2.41 | 633   | 0 | 0% | 159.65  | 0 | 822  | 69  |  504| 724 | 9.31| 39.41 |8.64|
| GET http://10.0.102.61:41424/api/v1/query?query=kube_node_info   | prometheus-3.2  | 599   | 0 | 0% | 164.09  | 0 | 1157 | 280 | 506 | 846 |9.2  | 33.33 |7.39|

**CPU 和内存使用占比对比验证**
![prometheus-4-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-4-2025-03-18.png)
![prometheus-5-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-5-2025-03-18.png)


**数据存储占比验证**
开发环境 prometheus-3.2,保存数据一天

```bash
[root@yigou-dev-102-69 prometheus]# du -sh prometheus-db/
2.9G    prometheus-db/
[root@yigou-dev-102-69 prometheus]# cd prometheus-db/
[root@yigou-dev-102-69 prometheus-db]# ll
total 884220
drwxr-xr-x  7 24152 root  10088448 Mar 10 17:00 01JNZMGP1RB9NGKME3NND81ZVK.tmp-for-deletion
drwxr-xr-x  7 24152 root  13611008 Mar 10 19:00 01JNZVCD9W8ZHVATSCWV6Z246A.tmp-for-deletion
drwxr-xr-x  7 24152 root  23740416 Mar 10 21:00 01JP0283VZAWP6EAQEYMW9N0GR.tmp-for-deletion
drwxr-xr-x  7 24152 root  23343104 Mar 10 23:00 01JP093V3KB66MF2WM35KA3Z48.tmp-for-deletion
drwxr-xr-x  7 24152 root  23728128 Mar 11 01:00 01JP0FZJBWFVK909RX960C3EFB.tmp-for-deletion
drwxr-xr-x  7 24152 root  23601152 Mar 11 03:00 01JP0PV9KGX1AK89AZ8JZEGF8V.tmp-for-deletion
drwxr-xr-x  7 24152 root  23343104 Mar 11 05:00 01JP0XQ0VG9PTY12NAB6VZBHSS.tmp-for-deletion
drwxr-xr-x  7 24152 root  23638016 Mar 11 07:00 01JP14JR407R27444EJ0PAX3WP.tmp-for-deletion
drwxr-xr-x  7 24152 root  23396352 Mar 11 09:00 01JP1BEFC0ZKH5YDX5GXMPXCYR.tmp-for-deletion
drwxr-xr-x  7 24152 root  23515136 Mar 11 11:00 01JP1JA6KN23K30FPV80Z4GS0F.tmp-for-deletion
drwxr-xr-x  7 24152 root  24088576 Mar 11 13:00 01JP1S5XKJTJ3079C3PNHDC3X8.tmp-for-deletion
drwxr-xr-x  7 24152 root  23207936 Mar 11 15:00 01JP201N3J5S9PE3798B6DSZ51.tmp-for-deletion
drwxr-xr-x  7  1000 root  23678976 Mar 16 19:00 01JPF9RPMPVKWD68FBR59KV0PR
drwxr-xr-x  7  1000 root  24154112 Mar 16 21:00 01JPFGMDWMF25WYXKMXDQ1RX3P
drwxr-xr-x  7  1000 root  23736320 Mar 16 23:00 01JPFQG54KZ61FA1NHJAZ0W442
drwxr-xr-x  7  1000 root  23728128 Mar 17 01:00 01JPFYBWCKTZ2RMVV3T7YS59N2
drwxr-xr-x  7  1000 root  23650304 Mar 17 03:00 01JPG57KMNG99D5RVPWYS6EDEB
drwxr-xr-x  7  1000 root  23719936 Mar 17 05:00 01JPGC3AWKDBJ6HEZ6TP1T2T7N
drwxr-xr-x  7  1000 root  24133632 Mar 17 07:00 01JPGJZ24P1QY1J86XEQP0R226
drwxr-xr-x  7  1000 root  23789568 Mar 17 09:00 01JPGSTSCQ8MT56M469G1CB100
drwxr-xr-x  7  1000 root  24100864 Mar 17 11:00 01JPH0PGF60AGP03VH2MCAD82F
drwxr-xr-x  7  1000 root  23691264 Mar 17 13:00 01JPH7J835WPHE9D94WYMKTT48
drwxr-xr-x  7  1000 root  24358912 Mar 17 15:00 01JPHEDZ4QFPMQEGKS2BY47D8C
drwxr-xr-x  7  1000 root  24371200 Mar 17 17:00 01JPHN9PCTDWFX8TESDYXP7B8S
drwxrwxrwx  5  1000 root  25370624 Mar 17 17:18 chunks_head
-rw-r--r--  1  1000 root         0 Mar 17 17:05 lock
-rwxrwxrwx  1  1000 root     20001 Mar 17 17:49 queries.active
drwxrwxrwx 11  1000 root 333635584 Mar 17 17:05 wal
```
测试环境 prometheus 2.41 保存数据一天
```bash
[root@yigou-stg-101-69 prometheus-db]# ll
total 3112404
drwxr-xr-x  7 1000 root 201785344 Mar 16 17:00 01JPF2X1AK4HR2R0FGJ4WJ9H5R
drwxr-xr-x  7 1000 root 181075968 Mar 16 19:00 01JPF9RRHPV2DF0DWCRDZ5V9S2
drwxr-xr-x  7 1000 root 183742464 Mar 16 21:00 01JPFGMFS3RP8BFK4XGNQREH4J
drwxr-xr-x  7 1000 root 180129792 Mar 16 23:00 01JPFQG7291835MQBGTS23GW5H
drwxr-xr-x  7 1000 root 172158976 Mar 17 01:00 01JPFYBY9FMX1BKRZN15V9WKRQ
drwxr-xr-x  7 1000 root 172908544 Mar 17 03:00 01JPG57NH0HAWB97E44P0F1RVH
drwxr-xr-x  7 1000 root 172462080 Mar 17 05:00 01JPGC3CWKEP8NJWZTNMXV9E7E
drwxr-xr-x  7 1000 root 170082304 Mar 17 07:00 01JPGJZ41GKKSBZQ0AJTVS2BGS
drwxr-xr-x  7 1000 root 170999808 Mar 17 09:00 01JPGSTVB06SGXBX5BQDQ5BVGJ
drwxr-xr-x  7 1000 root 172638208 Mar 17 11:00 01JPH0PJMFXJN2CTKRCJPHGHEG
drwxr-xr-x  7 1000 root 168775680 Mar 17 13:00 01JPH7J9VNXPXAW5YJA2MZKY59
drwxr-xr-x  7 1000 root 170446848 Mar 17 15:00 01JPHEE14772BRPFEKYJ93ZEKE
drwxr-xr-x  7 1000 root 173338624 Mar 17 17:00 01JPHN9R9MTRX0TWKAE574Y0V6
drwxr-xr-x  5 1000 root 165650432 Mar 17 17:00 chunks_head
-rw-r--r--  1 1000 root         0 Mar  6 15:21 lock
-rw-r--r--  1 1000 root     20001 Mar 17 17:53 queries.active
drwxr-xr-x 10 1000 root 730886144 Mar 17 17:27 wal
[root@yigou-stg-101-69 prometheus-db]# cd ..
[root@yigou-stg-101-69 monitoring-prometheus]# du -sh prometheus-db/
11G     prometheus-db/
```
#### 带数据升级Prometheus 3.2版本验证

复制后的数据需要添加权限，否则无法读取。
```bash
scp -r prometheus-db/*  10.0.102.61:/user-storage/gbasic-component/monitoring/prometheus/prometheus-db
cd /user-storage/gbasic-component/monitoring/prometheus/prometheus-db
chmod -R 777 *
```

数据从测试环境复制到开发环境，验证数据是否能正常读取?

验证结果如下：

![dataset-1-2025-03-21](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/dataset-1-2025-03-21.png)

算法环境的指标数据，在开发环境中是否正常读取？

验证结果如下：
![dataset-2-2025-03-21](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/dataset-2-2025-03-21.png)


## 多云监控方案

### 背景现状

方案涉及的自定义和开源组件如下：
- kube-state-metrics：提供K8S CR状态信息
- node-exporter: 提供节点的网络、存储、CPU、内存、磁盘等指标。
- dcgm-exporter：提供GPU相关指标。
- cadvisor(kubelet) ： 提供容器的网络、存储、CPU、内存、磁盘等指标。
- prometheus：监控数据存储、查询、告警。
- grafana：监控数据可视化;提供grafana相关监控指标。
- resource-manager：提供监控api接口，监控数据后处理。
- kubernetes-pods：pod 级别的自动发现，提供istio 推理的监控指标。
- minio：提供对象存储相关监控指标。
- alertmanager：提供告警相关指标。
- coredns：提供dns 相关指标。
- hero-exporter：提供gpu卡时相关指标。
- kube-apiserver: 提供apiserver相关指标。
- kube-controller-manager: 提供controller-manager相关指标。
- kube-scheduler: 提供scheduler相关指标。
- kube-proxy: 提供proxy相关指标。
- kube-etcd: 提供etcd相关指标。
- prometheus-operator: 提供prometheus-operator相关指标。

当前单集群监控部署监控方案架构如下：
![sig-1-2025-03-21](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/sig-1-2025-03-21.png)


**存在的问题**：
- 监控指标没有分流，数据存储压力大。


当前多云集群监控部署方案架构如下：

![mutli-1-2025-03-21](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/mutli-1-2025-03-21.png)

**存在的问题**：
- 监控数据存储在每个子集群中。没有办法进行统一存储。统一查询。以及存在短期数据丢失的风险。

### 解决方案

#### 单集群部署方案-数据分流
![sig-2-2025-03-24](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/sig-2-2025-03-24.png)

**实施流程**：
- 根据job来进行数据分流，user-prometheus配置当前用户端使用的监控指标数据来源，具体exporter如下：
  - node-exporter
  - kube-state-metrics 
  - kubelet(cadvisor) 
  - dcgm-exporter 
  - npu-exporter
  - kubernetes-pods
- system-Prometheus配置剩余exporter，提供短期的数据查询。
- resource-manager只对接user-prometheus。
- grafana对接user-prometheus和system-prometheus。

**方案结论**：
#TODO
  
#### 多云集群部署方案-VictoriaMetrics

通过比较社区活跃度，部署难度，性能分析，和运维难度等方面来进行对比，在多云主控下部署`VictoriaMetrics`方案最优，对比结果如下表：

| 维度	| VictoriaMetrics | Thanos | Cortex | Grafana Mimir |
| --- | --- | --- | --- | --- |
| 性能	| 写入与查询性能最优，单节点支持百万级数据点/秒	| 查询性能受对象存储延迟影响，高基数场景易出现瓶颈	| 类似 Thanos，依赖对象存储，性能中等	| 与 Thanos 架构类似，优化查询缓存，性能略优于 Thanos |
| 存储效率 | 压缩率最高（比 Prometheus 减少 70% 存储）	| 依赖对象存储，压缩率一般	| 同 Thanos，存储成本较高 | 类似 Thanos，但支持更高效的分块存储 |
| 架构复杂度 | 组件少（vminsert/vmselect/vmstorage），部署简单	| 组件多（Sidecar/Store/Query 等），运维复杂 | 	与 Thanos 类似，组件较多| 	类似 Thanos，但优化了组件交互逻辑 |
| 扩展性 | 水平扩展灵活，存储与计算分离 | 依赖 Prometheus 实例扩展，对象存储扩展性受限 | 水平扩展能力中等 | 支持动态分片，扩展性优于 Thanos |
| 成本	| 本地存储成本低，流量成本低 |	对象存储成本低，但跨区流量费用高| 同 Thanos	| 存储成本类似 Thanos，企业版许可费用较高 |
| 可靠性	| 数据强一致性，宕机时数据丢失风险低（秒级）| 数据上传周期长（2小时），宕机可能丢失较多数据 | 同 Thanos	 | 优化了数据块合并策略，可靠性略高 |
| 生态兼容性 | 完全兼容 Prometheus API，支持多数据源（InfluxDB/Graphite 等）| 仅支持 Prometheus 生态 | 仅支持 Prometheus 生态 | 深度集成 Grafana，兼容 Prometheus |

#### 部署场景-主控对接多个子集群

> 单集群监控组件仍然选择prometheus的原因: 具有可视化的页面，方便查看监控数据的状态以及问题定位

(1) 子集群不提供对外服务，只提供资源。
![mutli-2-2025-03-24](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/mutli-2-2025-03-24.png)

(2) 子集群和主控集群都提供服务。
![mutli-3-2025-03-24](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/mutli-3-2025-03-24.png)


**方案优势**：
子集群监控数据统一存储在主控集群中，便于监控数据的统一管理和查询。VictoriaMetrics高可用架构，可以保证监控数据的可靠性和高效性。存储压缩效率高，存储压力更小。

**需要考虑的点**：
- 需要考虑多个集群不在同一局域网下，如何使用公网ip来配置remotewrite ？何种方式更新上面配置？

**实施流程**：

1. 子集群Prometheus CR配置修改（应用测修改）
    1. 添加 remotewrite配置参数，指向主控的监控组件。
    ```bash
    remoteWrite:
        - url: http://vm-cluster-victoria-metrics-cluster-vminsert.vm.svc.cluster.local:8480/insert/0/prometheus # vm-cluster
    ```
    2. 添加external_labels配置参数，唯一标识每个监控集群。
    ```bash
    external_labels:
        clusterIP: localhost
    ```
2. Resource-Manager (系统测修改)
   1. 监控指标查询接口的方式需要从Prometheus sdk改成Http服务请求。
   2. 某些查询指标需要进行调整。
  
3. Grafana 修改项（系统测和前端修改）
   1. grafana 面板需要添加cluster参数，因子集群数据直接同步到主控集群，所以可以不通过数据源的方式从子集群Prometheus进行查询。前端接口需要进行调整。
   2. 某些查询指标需要进行调整。


#### 部署场景-单子集群对接多个主控集群

**存在的问题**：
单个子集群对接多个主控集群的时候，监控数据如何分流到不同的主控集群中？

**方案1 根据命名空间实现分流**

不同主控集群的任务下发到不同的用户命名空间，然后Prometheus收集所有监控数据，根据命名空间进行分流。
例如：
- 主控集群A：hero-user-clsuterA_ID
- 主控集群B：hero-user-clsuterB_ID

Prometheus CR调整如下：
```bash
    remoteWrite:
    - url: http://remote-write-1.example.com/api/v1/write # 集群A
      writeRelabelConfigs:
        - sourceLabels: [__meta_kubernetes_namespace]
          regex: hero-user-clsuterB_ID # 排除集群B命名空间的数据
          action: drop
    - url: http://remote-write-2.example.com/api/v1/write # 集群B 
      writeRelabelConfigs:
        - sourceLabels: [__meta_kubernetes_namespace]
          regex: hero-user-clsuterA_ID # 排除集群A命名空间的数据
          action: drop
```
如何实现CR的更新？

结构图如下：
![mutli-4-2025-03-24](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/mutli-4-2025-03-24.png)
实现较为简单，但需要修改karmada下发任务的逻辑。不能统一下发任务到hero-user命名空间。


**方案2 根据pod label实现分流**
将pod的label写入所有的监控指标中，然后通过label进行数据分流。
例如：
- 主控集群A：label: cluster_id=clsuterA_ID
- 主控集群B：label: cluster_id=clsuterB_ID

Prometheus CR调整类似方案1。

意味着所有的指标采集exporter都需要将pod label作为指标标签。 主要涉及cadvisor(kubelet)、dcgm-exporter、kube_state_metrics。
- cadvisor(kubelet) 社区明确标识不支持将label作为指标标签，因为label是变动的，会影响序列的连续性。可以修改源码实现。
- kube_state_metrics 可以通过配置开启将pod label作为指标标签。
- dcgm-exporter 不支持配置，需要修改源码实现。


不建议采取该方案，工作量较大。后续增加exporter的时候都需要去适配该功能。

### 开源技术选型
#### Grafana Mimir
##### 功能特性
1. `易维护性`：Grafana Mimir 的核心优势之一便是易于“安装和维护”。该项目的大量文档、教程和部署工具使其入门起来既快速又简单。Grafana Mimir 的整体模式允许只使用一个二进制文件，不需要额外的依赖项。此外，与 Grafana Mimir 一起打包的最佳实践仪表板、警报和运行手册可以轻松监控系统的健康状况并保持其平稳运行。
2. `可扩展性`：Grafana Mimir 的水平可扩展架构使其能够处理大量时间序列数据。内部测试表明，该系统可以处理多达 10 亿个活动时间序列，从而实现大规模的可扩展性。这意味着 Grafana Mimir 可以跨多台机器运行，从而能够处理比单个 Prometheus 实例多几个数量级的时间序列。
3. `全局视图`：Grafana Mimir 的另一个关键优势是它能够提供全局的指标视图。该系统使用户能够运行聚合来自多个 Prometheus 实例的系列的查询，从而提供所有系统的全面视图。查询引擎还广泛并行化查询执行，即使是最高基数的查询也能以极快的速度执行。
4. `数据持久性`：Grafana Mimir 使用对象存储来进行长期数据存储，利用了这种无处不在的、高性价比、高耐久性的技术。该系统与多个对象存储实现兼容，包括 AWS S3、谷歌云存储、Azure Blob 存储、OpenStack Swift 以及任何与 S3 兼容的对象存储。这为用户提供了一种廉价、耐用的方式来存储用于长期分析的指标。
5. `通过复制实现高可用性`：高可用性是 Grafana Mimir 的另一个关键特性。系统复制传入的指标，确保在机器发生故障时不会丢失任何数据。其水平可扩展架构还意味着它可以在零停机的情况下重启、升级或降级，确保指标摄取或查询不会中断。
6. `原生多租户`：Grafana Mimir 的原生多租户允许独立团队或业务部门的数据和查询隔离，使这些组可以共享同一个集群。高级限制和服务质量控制确保容量在租户之间公平共享，使其成为拥有多个团队和部门的大型组织的绝佳选择。

#####  服务部署
helm部署仓库：[mimir/operations/helm at main · grafana/mimir](https://github.com/grafana/mimir/tree/main/operations/helm)
```bash
[root@localhost thanos]# kubectl get pod -n mimir 
NAME                                        READY   STATUS             RESTARTS   AGE
mimir-alertmanager-0                        1/1     Running            0          3d1h
mimir-compactor-0                           1/1     Running            0          3d1h
mimir-distributor-55d4d6c56-pbj99           1/1     Running            0          3d1h
mimir-ingester-zone-a-0                     1/1     Running            0          3d1h
mimir-ingester-zone-b-0                     1/1     Running            0          3d1h
mimir-ingester-zone-c-0                     1/1     Running            0          3d1h
mimir-make-minio-buckets-5.4.0-5tbz5        0/1     Completed          0          3d1h
mimir-minio-57c55c679-rpmkw                 1/1     Running            0          3d1h
mimir-nginx-6f77d6f78-fkx9w                 1/1     Running            0          3d1h
mimir-overrides-exporter-6d97f959c5-sqwbq   1/1     Running            0          3d1h
mimir-querier-5fffbb78bd-588wn              1/1     Running            0          3d1h
mimir-querier-5fffbb78bd-8vz2g              1/1     Running            0          3d1h
mimir-query-frontend-69dbcf44c5-6qmvz       1/1     Running            0          3d1h
mimir-query-scheduler-698bdc98d6-9vg9n      1/1     Running            0          3d1h
mimir-query-scheduler-698bdc98d6-bs4vk      1/1     Running            0          3d1h
mimir-rollout-operator-744b68588-pjngz      0/1     Running            0          3d1h
mimir-ruler-7959b6bdd9-68jjh                1/1     Running            0          3d1h
mimir-store-gateway-zone-a-0                1/1     Running            0          3d1h
mimir-store-gateway-zone-b-0                1/1     Running            0          3d1h
mimir-store-gateway-zone-c-0                1/1     Running            0          3d1h
```
#####   服务架构

![mimir-1-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/mimir-1-2025-03-18.png)

- `Distributor`：Distributor 是一个无状态组件，它从 Prometheus 或 Grafana Alloy 接收时间序列数据。然后，Distributor 将数据分成批次，并并行发送到多个 ingester，在 ingester 之间分片序列，并按配置的复制因子复制每个序列。
- `Ingester`： 接受Distributor传入的样本，并将接收到的样本附加到存储在本地磁盘上的特定于租户的 TSDB，每两小时会定期刷新到磁盘，将新创建的块都会上传到长期存储。
- `Compactor`：将来自多个 Ingester的块合并到一个块中，并删除重复的样本。块压缩显着降低了存储利用率。
  - 垂直 Compaction 将 Ingester 同一时间范围（默认情况下为 2 小时范围）上传的租户的所有块合并为单个块。它还会删除最初写入 N 个块的样本的重复数据，这是复制的结果。垂直 Compaction 将单个时间范围的块数从 Ingester 的数量减少到每个租户一个块。
  - 水平 Compaction 在垂直 Compaction 后触发。它将具有相邻范围周期的多个块压缩成单个更大的块。水平 Compaction 后，关联块数据块的总大小不会改变。水平 Compaction 可以显着减小 Store-gateway 保存在内存中的索引和 index-header 的大小。
- `Query-frontend`：将较长时间范围的查询拆分为多个较小的查询。如果查询结果已缓存，则 Query-frontend（查询前端）返回缓存的结果。无法从结果缓存回答的查询将放入 Query-frontend内的内存队列中。此外聚合Querier查询的数据结果。

- `Querier`：充当 worker，从队列中拉取查询，并根据查询范围向每个匹配的 store-gateway 实例发送请求。

- `Store-gateway`：store-gateway 需要长期存储中 bucket 的视图。store-gateway 通过定期下载 bucket 索引 来保持 bucket 视图更新; 块分片和复制,块在多个 store-gateway 实例之间复制,块复制用于防止因某些块在给定时间未被任何 store-gateway 实例加载而导致的查询失败，例如在 store-gateway 故障或重启 store-gateway 实例时（例如，在滚动更新期间）。Store-gateway 加载属于其 store-gateway 分片的每个块的索引头。在 store-gateway 加载块的索引头后，该块即可被 querier 查询。当 querier 通过 store-gateway 查询块时，响应包含查询的块 ID 列表。如果 querier 尝试查询 store-gateway 未加载的块，则 querier 会在不同的 store-gateway 上重试查询，最多达到 -store-gateway.sharding-ring.replication-factor 值，默认值为 3。如果无法从任何副本成功查询块，则查询失败。参考 [Grafana Mimir store-gateway | Grafana Mimir 文档 - Grafana](https://grafana.org.cn/docs/mimir/latest/references/architecture/components/store-gateway/)

- `Alertmanager`: 可选组件，接受来自 Mimir ruler 的警报通知。Alertmanager 对警报通知进行去重和分组，并将它们路由到通知通道，例如电子邮件、PagerDuty 或 OpsGenie。

- `Overrides-exporter`: 防止单个租户使用过多的资源,例如速率、series总量等。
  
- `Query-scheduler`： 是一个可选的、无状态的组件，它保留一个待执行的查询队列，并将工作负载分配给可用的 queriers。参考 [Grafana Mimir query-scheduler](https://grafana.org.cn/docs/mimir/latest/references/architecture/components/query-scheduler/)
    ![image-4-2025-03-19](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/image-4-2025-03-19.png)

- `Ruler`: 是一个可选组件，用于评估记录和告警规则中定义的 PromQL 表达式。每个租户都有一组记录和告警规则，并且可以将这些规则分组到命名空间中。

#### Thanos

#####  功能特性

- `对象存储集成`：可以将 Prometheus 数据存储在对象存储中，如 S3、GCS 等，解决了 Prometheus 本地存储的容量限制问题，实现数据的长期存储和高可用性。
- `全局查询`：通过 Querier 组件，能够对多个 Prometheus 实例的数据进行全局查询和聚合，提供统一的查询入口，方便用户在大规模分布式环境中查询和分析监控数据。
- `数据复制与一致性`：支持数据的复制和一致性保证，确保在多个 Prometheus 实例和存储之间数据的准确性和完整性，提高了数据的可靠性。
- `水平扩展`：可以轻松地进行水平扩展，通过添加更多的组件实例来处理不断增长的监控数据量和查询请求，满足大规模集群监控的需求。
- `与 Prometheus 集成`：与 Prometheus 紧密集成，能够无缝地读取 Prometheus 的指标数据，并且可以利用 Prometheus 的现有生态和配置，降低了用户的使用成本和学习门槛。

##### 安装部署
Thanos Receiver安装部署

部署仓库：https://github.com/thanos-io/kube-thanos

```bash
thanos-query-6bc44896cd-bnk7t            1/1     Running   0          4d5h
thanos-receive-ingestor-default-0        1/1     Running   0          4d5h
thanos-receive-router-65df965665-bhfcr   1/1     Running   0          4d5h
thanos-store-0                           1/1     Running   0          4d5h
```

##### 服务架构
存储使用： 对象存储， 文件存储（测试阶段，不建议生产环境使用）

**Reciver模式**

![thanos-1-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/thanos-1-2025-03-18.png)

**Sidecar模式**
![thanos-2-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/thanos-2-2025-03-18.png)

- `Sidecar`：连接到Prometheus，读取其数据进行查询和/或将其上传到云存储。
- `Store Gateway`：提供云存储桶内部的指标。
- `Compactor`：对存储在云存储桶中的数据进行压缩、下采样和保留。
- `Receiver`：接收来自Prometheus远程写提前写日志的数据，将其公开，并/或将其上传到云存储。
- `Ruler/Rule`：针对prometheus中的数据评估记录和警报规则，以便展示和/或上传。
- `Querier/Query`：实现Prometheus的v1 API，从底层组件聚合数据。
- `Query Frontend`：实现Prometheus的v1 API，将其代理到Querier，同时缓存响应，并可选择按每天的查询拆分它。

#### VictoriaMetrics
Prometheus 的长期存储或作为 Grafana 中 Prometheus 和 Graphite 的替代品。

Api接口文档：[查询 API – VictoriaMetrics 中文手册](https://victoriametrics.com.cn/docs/query/api/)

#### 功能特性
- 兼容 PromQL，直接替代 Prometheus 存储。
- 支持 Prometheus 远程读写、长期存储（本地或云存储）。
- `最小内存占用`：处理数百万个独特的时间序列，所需内存比 InfluxDB 少 10 倍，比 Prometheus、Thanos 或 Cortex 少最多 7 倍。
- `高度可扩展性和数据摄取与查询性能`: 性能比 InfluxDB 和 TimescaleDB 高 20 倍。
- `高数据压缩`：存储点数比 TimescaleDB 多 70 倍的有限存储空间，所需存储空间比 Prometheus、Thanos 或 Cortex 少 7 倍。
- `降低存储成本`：根据 Grammarly 案例研究，比 Graphite 高效 10 倍。
- `单节点 VictoriaMetrics 可替代中等规模的集群`:这些集群是使用 Thanos、M3DB、Cortex、InfluxDB 或 TimescaleDB 等竞争解决方案构建的。参见 VictoriaMetrics 与 Thanos、衡量垂直可扩展性、远程写入存储战争 - PromCon 2019。
- `优化存储`：在高延迟 IO 和低 IOPS（HDD 和 AWS、Google Cloud、Microsoft Azure 等的网络存储）上运行良好。

#### 服务架构
![vm-1-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/vm-1-2025-03-18.png)

#### 服务组件
VictoriaMetrics生态系统除了单节点的VictoriaMetrics外，还包含以下组件：
- `vmagent` - 一个轻量级代理，用于通过基于拉取和推送协议接收指标，对其进行转换并发送到已配置的与 Prometheus 兼容的远程存储系统（如 VictoriaMetrics）。
- `vmalert` - 用于处理与 Prometheus 兼容的告警和记录规则的服务。
- `vmalert-tool` - 用于验证告警和记录规则的工具。
- `vmauth` - 针对 VictoriaMetrics 产品优化的授权代理和负载均衡器。
- `vmgateway` - 带有按租户速率限制功能的授权代理。
- `vmctl` - 用于在不同存储系统之间迁移和复制指标数据的工具。
- `vmbackup`、`vmrestore` 和 `vmbackupmanager` - 用于为VictoriaMetrics指标数据创建备份和从备份恢复的工具。
- `vminsert`、`vmselect` 和 `vmstorage` - VictoriaMetrics集群的组件。
    - `vmstorage` - 存储原始数据，并在给定的时间范围内返回符合指定标签过滤器的数据  
    - `vminsert` - 接收摄入的数据，并根据指标名称及其所有标签的一致性哈希算法将其分发到 vmstorage 节点中  
    - `vmselect` - 通过从所有已配置的 vmstorage 节点中获取所需数据来执行传入查询
- `VictoriaLogs` - 用户友好且成本效益高的日志数据库。

#### 安装部署
单机版ui：http://10.0.102.95:32494/vmui/
集群版ui：http://10.0.102.94:30787/select/0/vmui/

单机版安装：[Kubernetes monitoring via VictoriaMetrics Single](https://docs.victoriametrics.com/guides/k8s-monitoring-via-vm-single/)
```bash
git clone https://github.com/VictoriaMetrics/helm-charts.git
cd vm/helm-charts/charts/victoria-metrics-single
helm dependency build
cd ..
helm install vmsingle victoria-metrics-single/

[root@localhost charts]# kubectl get pod 
NAME                                        READY   STATUS    RESTARTS   AGE
vmsingle-victoria-metrics-single-server-0   1/1     Running   0          3m20s
```

集群版安装：[Kubernetes monitoring with VictoriaMetrics Cluster](https://docs.victoriametrics.com/guides/k8s-monitoring-via-vm-cluster/)
```bash
vm-cluster-victoria-metrics-cluster-vminsert-75c498d4f6-57k5c   1/1     Running   0          6d18h
vm-cluster-victoria-metrics-cluster-vminsert-75c498d4f6-fpwvr   1/1     Running   0          6d18h
vm-cluster-victoria-metrics-cluster-vmselect-558957f8f5-rszgm   1/1     Running   0          22h
vm-cluster-victoria-metrics-cluster-vmselect-558957f8f5-rvzzr   1/1     Running   0          6d18h
vm-cluster-victoria-metrics-cluster-vmselect-558957f8f5-tlpxq   1/1     Running   0          6d18h
vm-cluster-victoria-metrics-cluster-vmstorage-0                 1/1     Running   0          6d16h
vm-cluster-victoria-metrics-cluster-vmstorage-1                 1/1     Running   0          6d16h

# 文件存储 
# 集群版
[root@localhost ~]# ll /opt/local-path-provisioner/pvc-ce818a4b-7dc9-4d9c-b2ed-c3dcf35edbe2_default_server-volume-vmsingle-victoria-metrics-single-server-0
total 0
drwxr-xr-x 6 root root 214 Mar 11 14:13 cache
drwxr-xr-x 4 root root  30 Mar 10 19:52 data
-rw-r--r-- 1 root root   0 Mar 11 14:13 flock.lock
drwxr-xr-x 6 root root  95 Mar 10 19:52 indexdb
drwxr-xr-x 2 root root  43 Mar 10 19:52 metadata
drwxr-xr-x 2 root root   6 Mar 10 19:52 snapshots
drwxr-xr-x 3 root root  27 Mar 11 14:13 tmp
# 单机版
[root@localhost ~]# ll /opt/local-path-provisioner/pvc-9cee66d9-24a5-4012-9de3-13c65e221339_vm_vmstorage-volume-vm-cluster-victoria-metrics-cluster-vmstorage-0
total 0
drwxr-xr-x 4 root root 30 Mar 11 15:24 data
-rw-r--r-- 1 root root  0 Mar 11 15:24 flock.lock
drwxr-xr-x 6 root root 95 Mar 11 15:24 indexdb
drwxr-xr-x 2 root root 43 Mar 11 15:24 metadata
drwxr-xr-x 2 root root  6 Mar 11 15:24 snapshots
```

#### Prometheus

**Prometheus 联邦**

![prometheus-6-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-6-2025-03-18.png)

**Prometheus Remote Write**

![prometheus-7-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/prometheus-7-2025-03-18.png)


####  Cortex
#TODO
####  M3DB
#TODO
####  PromscaleDB
#TODO
####  TimescaleDB
#TODO

### 方案功能测试对比

#### 指标查询接口响应格式对比

- Prometheus
  
```json
    {
        "status": "success",
        "data": {
            "resultType": "vector",
            "result": [
                {
                    "metric": {
                        "__name__": "kube_node_info",
                        "container": "kube-rbac-proxy-main",
                        "container_runtime_version": "docker://24.0.7",
                        "instance": "10.244.58.97:8443",
                        "internal_ip": "10.0.102.63",
                        "job": "kube-state-metrics",
                        "kernel_version": "5.10.0-153.12.0.92.oe2203sp2.x86_64",
                        "kubelet_version": "v1.25.14-heroes",
                        "kubeproxy_version": "deprecated",
                        "node": "yigou-dev-102-63",
                        "os_image": "openEuler 22.03 (LTS-SP2)",
                        "pod_cidr": "10.244.1.0/24",
                        "system_uuid": "4f753042-aa1f-687e-4e88-a6f7653a2208"
                    },
                    "value": [
                        1742266951.102,
                        "1"
                    ]
                }
            ]
        }
    }
```
- Mimir
  
```json
    {
        "status": "success",
        "data": {
            "resultType": "vector",
            "result": [
                {
                    "metric": {
                        "__name__": "kube_node_info",
                        "cluster": "dev-69",
                        "container": "kube-rbac-proxy-main",
                        "container_runtime_version": "containerd://2.0.1",
                        "instance": "10.244.58.97:8443",
                        "internal_ip": "10.0.102.64",
                        "job": "kube-state-metrics",
                        "kernel_version": "5.10.0-153.12.0.92.oe2203sp2.x86_64",
                        "kubelet_version": "v1.25.14-heroes",
                        "kubeproxy_version": "deprecated",
                        "node": "yigou-dev-102-64",
                        "os_image": "openEuler 22.03 (LTS-SP2)",
                        "pod_cidr": "10.244.9.0/24",
                        "prometheus": "monitoring/k8s",
                        "prometheus_replica": "prometheus-k8s-0",
                        "system_uuid": "91b33042-ff86-795c-00bc-f8359c8f14d7"
                    },
                    "value": [
                        1742201541.75,
                        "1"
                    ]
                }
            ]
        }
    }
```
- Thanos

```json
    {
        "status": "success",
        "data": {
            "resultType": "vector",
            "result": [
                {
                    "metric": {
                        "__name__": "kube_node_info",
                        "cluster": "dev-69",
                        "container": "kube-rbac-proxy-main",
                        "container_runtime_version": "containerd://2.0.1",
                        "instance": "10.244.58.97:8443",
                        "internal_ip": "10.0.102.64",
                        "job": "kube-state-metrics",
                        "kernel_version": "5.10.0-153.12.0.92.oe2203sp2.x86_64",
                        "kubelet_version": "v1.25.14-heroes",
                        "kubeproxy_version": "deprecated",
                        "node": "yigou-dev-102-64",
                        "os_image": "openEuler 22.03 (LTS-SP2)",
                        "pod_cidr": "10.244.9.0/24",
                        "prometheus": "monitoring/k8s",
                        "receive": "true",
                        "replica": "thanos-receive-ingestor-default-0",
                        "system_uuid": "91b33042-ff86-795c-00bc-f8359c8f14d7",
                        "tenant_id": "default-tenant"
                    },
                    "value": [
                        1742201642.086,
                        "1"
                    ]
                }
            ]
        }
    }
```
- VictoriaMetrics
  
```json
    {
        "status": "success",
        "isPartial": false,
        "data": {
            "resultType": "vector",
            "result": [
                {
                    "metric": {
                        "__name__": "kube_node_info",
                        "cluster": "dev-69",
                        "container": "kube-rbac-proxy-main",
                        "container_runtime_version": "docker://24.0.7",
                        "instance": "10.244.58.97:8443",
                        "internal_ip": "10.0.102.60",
                        "job": "kube-state-metrics",
                        "kernel_version": "5.10.0-153.56.0.134.oe2203sp2.x86_64",
                        "kubelet_version": "v1.25.14-heroes",
                        "kubeproxy_version": "deprecated",
                        "node": "yigou-dev-102-61",
                        "os_image": "openEuler 22.03 (LTS-SP2)",
                        "pod_cidr": "10.244.0.0/24",
                        "prometheus": "monitoring/k8s",
                        "prometheus_replica": "prometheus-k8s-0",
                        "system_uuid": "d6eb3042-b2bd-8d0d-e811-6728f8bf6f06"
                    },
                    "value": [
                        1742201708,
                        "1"
                    ]
                }
            ]
        },
        "stats": {
            "seriesFetched": "12",
            "executionTimeMsec": 29
        }
    }
```

#### 指标查询性能对比

1. 50并发一分钟压力测试（数据量12条）

| 接口 | 名称 | 总请求数 | 失败数 | 失败率 | 平均响应时间(ms) | 最小响应(ms) | 最大响应(ms) | 90% (ms) | 95% (ms) | 99% (ms) | TPS | 平均接收流量| 每秒请求数 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET http://10.0.102.94:32566/prometheus/api/v1/query?query=kube_node_info| mimir | 576 | 0 | 0% | 145.91 | 0 | 3310 | 166 | 578| 2959 | 10.71 |63.71 | 9.09|
| GET http://10.0.102.94:31763/api/v1/query?query=kube_node_info| thanos | 543 | 0 | 0% | 139.27 | 0 | 4258 | 131 | 518 | 2406 | 10.05 | 65.51 | 8.6|
| GET http://10.0.102.94:30787/select/0/prometheus/api/v1/query?query=kube_node_info| cluster-vm | 552 | 0 | 0% | 153.73 | 0 | 4003 | 236 | 900 | 2654 | 10.17 | 61.63 | 8.76 |
| GET http://10.0.102.94:32494/prometheus/api/v1/query?query=kube_node_info| sig - vm | 545 | 0 | 0% | 88.95 | 0 | 3645 | 13 | 401 | 1866 | 10.08 | 60.88 | 8.68 |

2. 50并发一小时压力测试（数据量2400条）

| 接口 | 名称 | 总请求数 | 失败数 | 失败率 | 平均响应时间(ms) | 最小响应(ms) | 最大响应(ms) | 90% (ms) | 95% (ms) | 99% (ms) | TPS | 平均接收流量| 每秒请求数 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET http://10.0.102.94:32566/prometheus/api/v1/query_range?query=kube_node_info&start=1742186980&end=1742189980&step=15s | mimir-范围查询 | 468 | 0 | 0% | 250.39 | 0 | 3425 | 461 | 1147| 2492 | 8.82 | 354.94 | 9.40|
| GET http://10.0.102.94:31763/api/v1/query_range?query=kube_node_info&start=1742186980&end=1742189980&step=15s | thanos-范围查询 | 522 | 0 | 0% | 401.7 | 43 | 3020 | 382 | 1722 | 2469 | 9.9 | 401.06 | 8.45|
| GET http://10.0.102.94:30787/select/0/prometheus/api/v1/query_range?query=kube_node_info&start=1742186980&end=1742189980&step=15s | cluster-vm-范围查询 | 480 | 0 | 0% | 337 | 0 | 3748 | 184 | 1510 | 3484 | 9.04 | 360.63 | 7.69 |
| GET http://10.0.102.94:32494/prometheus/api/v1/query_range?query=kube_node_info&start=1742186980&end=1742189980&step=15s | sig-vm-范围查询 | 525 | 0 | 0% | 316.31 | 25 | 4086 | 829 | 1489 | 3070 | 9.56 | 398.02 | 8.49 |


3. 50并发一分钟压力测试（数据量2600条）复杂语句

| 接口 | 名称 | 总请求数 | 失败数 | 失败率 | 平均响应时间(ms) | 最小响应(ms) | 最大响应(ms) | 90% (ms) | 95% (ms) | 99% (ms) | TPS | 平均接收流量| 每秒请求数 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GET http://10.0.102.94:32566/prometheus/api/v1/query_range?query=sum(kube_pod_status_phase{phase="Running"} * on (pod) group_left(namespace, resource) sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)) by (resource)&start=1742150980&end=1742189980&step=15s | mimir-范围查询-复杂语句 | 518 | 0 | 0% | 1116.39 | 462 | 3660 | 1292 | 2407| 3341 | 9.51 | 701.36 | 8.31 |
| GET http://10.0.102.94:31763/api/v1/query_range?query=sum(kube_pod_status_phase{phase="Running"} * on (pod) group_left(namespace, resource) sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)) by (resource)&start=1742150980&end=1742189980&step=15s | thanos-范围查询-复杂语句 | 381 | 0 | 0% | 5687.61 | 2240 | 8653 | 5810 | 6531 | 7846 | 6.9 | 469.76 | 6.26 |
| GET http://10.0.102.94:30787/select/0/prometheus/api/v1/query_range?query=sum(kube_pod_status_phase{phase="Running"} * on (pod) group_left(namespace, resource) sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)) by (resource)&start=1742150980&end=1742189980&step=15s | cluster-vm-范围查找-复杂语句 | 521 | 0 | 0% | 572.9 | 0 | 3480 | 613 | 1703 | 2879 | 9.63 | 707.54 | 8.37 |
| GET http://10.0.102.94:32494/prometheus/api/v1/query_range?query=sum(kube_pod_status_phase{phase="Running"} * on (pod) group_left(namespace, resource) sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod)) by (resource)&start=1742150980&end=1742189980&step=15s | sig-vm-范围查询-复杂语句 | 512 | 0 | 0% | 558.05 | 0 | 2871 | 936 | 1804 | 2314 | 9.66 | 701.68 | 8.29 |

#### 存储空间占用对比

| 名称 | 存储方式 | 存储用量 |
| --- | --- | --- |
| mimir | 对象存储 | 928Mi |
| thanos | 对象存储 | 数据无法写入对象存储 |
| cluster-vm | 文件存储 | 135Mi + 155Mi |
| sig-vm | 文件存储 | 438M |
| prometheus-local | 文件存储 | 609M |
| prometheus-dev69 | 文件存储 | 2.9G | 

#### Grafana 面板集成可行性验证

![grafana-1-2025-03-18](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/grafana-1-2025-03-18.png)

对比服务都可与grafana面板进行集成

## 提高网络指标采集频率

### 提高相关exporter的采集频率

单纯的提高网络监控的exporter的采集频率，对Prometheus本身增加了很大的存储压力和性能压力。
```bash
time=2025-03-21T09:10:26.836Z level=ERROR source=queue_manager.go:1670 msg="non-recoverable error" component=remote remote_name=d09508 url=http://mimir-nginx.mimir.svc:80/api/v1/push failedSampleCount=2000 failedHistogramCount=0 failedExemplarCount=0 err="server returned HTTP status 429 Too Many Requests: the request has been rejected because the tenant exceeded the ingestion rate limit, set to 10000 items/s with a maximum allowed burst of 200000. This limit is applied on the total number of samples, exemplars and metadata received across all distributors (err-mimir-tenant-max-ingestion-rate). To adjust the related per-tenant limits, configure -distributor.ingestion-rate-limit and -distributor.ingestion-burst-size, or contact your service administrator.\n
```
发送速率超过10000 item/s，导致发送数据失败。

### 自定义网络exporter

构建秒级监控exporter服务

当前实现方案：通过将node-exporter中采集网络相关的监控指标的逻辑抽取出来，并通过秒级定时任务去采集数据和时间戳到缓存中，然后通过http接口暴露数据，供Prometheus采集定时采集。

可以根据后期需求来动态调整不同类指标的采集间隔，比如网络指标可以调整为1s，cpu指标可以调整为10s，磁盘指标可以调整为10s等。

代码仓库：https://gitlab.bitahub.com/hero-os/hero-exporter/-/tree/dev?ref_type=heads


## 参考文档

[Prometheus多集群监控的3种方案，你选哪种？_后端_华为云开发者联盟_InfoQ写作社区](https://xie.infoq.cn/article/05daf94e28fdfe76ce18f6ef1)
[Scale Prometheus：K8s 部署 GreptimeDB 集群作为 Prometheus 长期存储 | Greptime](https://www.greptime.cn/blogs/2024-09-26-scale-prometheus)
[Thanos 与 VictoriaMetrics，谁才是打造大型 Prometheus 监控系统的王者？-腾讯云开发者社区-腾讯云 (tencent.com) ](https://cloud.tencent.com/developer/article/1690227)
[prometheus高可用解决方案（VictoriaMetrics ）_prometheus 高可用-CSDN博客](https://blog.csdn.net/litaimin/article/details/141318450)
[使用 Thanos 集中管理多 Prometheus 实例数据 - 文章 - 开发者社区 - 火山引擎]([volcengine.com](https://developer.volcengine.com/articles/7391689032658321446))
[部署 Thanos Receive 模式实现 Prometheus 多集群管理-CSDN博客](https://developer.volcengine.com/articles/7391689032658321446)
[kube-prometheus/manifests at main · prometheus-operator/kube-prometheus](https://github.com/prometheus-operator/kube-prometheus/tree/main/manifests)
[helm-charts/charts/kube-prometheus-stack/values.yaml at main · prometheus-community/helm-charts](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml)
[使用 MinIO 与 Grafana Mimir 实现指标持久化存储-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2314693)
[Get started with Grafana Mimir and GEM using the Helm chart | Grafana Labs Helm charts documentation](https://grafana.com/docs/helm-charts/mimir-distributed/latest/get-started-helm-charts/)
[APO使用场景之：统一的指标采集展示Grafana Alloy介绍 Alloy是Grafana 发布替代之前Grafan - 掘金](https://juejin.cn/post/7413274784484229132)
[VictoriaMetrics 单点模式_victoriametrics docker-CSDN博客](https://blog.csdn.net/qq_34556414/article/details/125621536)
[VictoriaMetrics 集群模式部署_victoriametrics 集群部署-CSDN博客](https://blog.csdn.net/qq_34556414/article/details/125722088)
[使用thanos、prometheus和minio打造高可用监控平台 - 简书](https://www.jianshu.com/p/45153d5b2a25)