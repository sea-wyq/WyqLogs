



https://lws.sigs.k8s.io/docs/examples/vllm/


registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124

验证节点

节点模型文件位置：


pod 验证

单机单卡


```bash

apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
    spec:
      # 直接指定节点名称，强制调度到该节点
      nodeName: system-test-100-16-gpu-3090
      containers:
      - name: vllm
        image: registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124
        command: ["/bin/sh", "-c"]
        args: [
          "vllm serve /model/meta-llama/Llama-3.2-1B-Instruct"
        ]
        ports:
          - containerPort: 8000
        volumeMounts:
        - name: model-volume
          mountPath: /model/meta-llama/Llama-3.2-1B-Instruct
      volumes:
      - name: model-volume
        hostPath:
          path: /root/wyq/model
          type: Directory
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-server
spec:
  selector:
    app.kubernetes.io/name: vllm
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
    
```


```bash
curl http://10.244.167.208/v1/modes
curl http://127.0.0.1:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "/model/meta-llama/Llama-3.2-1B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
      }'
```

```bash
root@vllm-server-58d5c9c6b6-7k82t:/sgl-workspace# curl http:/127.0.0.1:8000/v1/models
{"object":"list","data":[{"id":"/model/meta-llama/Llama-3.2-1B-Instruct","object":"model","created":1754882768,"owned_by":"vllm","root":"/model/meta-llama/Llama-3.2-1B-Instruct","parent":null,"max_model_len":131072,"permission":[{"id":"modelperm-d8d1abd7da494d759ab5bacb68a6e338","object":"model_permission","created":1754882768,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}

root@vllm-server-58d5c9c6b6-7k82t:/sgl-workspace# curl http://127.0.0.1:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "/model/meta-llama/Llama-3.2-1B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
      }'
{"id":"cmpl-b6884a77fc614038b91fe627e0193c56","object":"text_completion","created":1754882699,"model":"/model/meta-llama/Llama-3.2-1B-Instruct","choices":[{"index":0,"text":" city that is full of life,","logprobs":null,"finish_reason":"length","stop_reason":null,"prompt_logprobs":null}],"usage":{"prompt_tokens":5,"total_tokens":12,"completion_tokens":7,"prompt_tokens_details":null}}
```

单机多卡

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
    spec:
      # 直接指定节点名称，强制调度到该节点
      nodeName: system-test-100-16-gpu-3090
      # 添加共享内存卷配置
      volumes:
      - name: model-volume
        hostPath:
          path: /root/wyq/model
          type: Directory
      - name: dshm  # 共享内存卷
        emptyDir:
          medium: Memory
          sizeLimit: 10Gi  # 10G共享内存
      containers:
      - name: vllm
        image: registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124
        command: ["/bin/sh", "-c"]
        args: [
          "vllm serve /model/meta-llama/Llama-3.2-1B-Instruct \
           --tensor-parallel-size 2 \
           --port 8000 \
           --host 0.0.0.0 \
           --gpu-memory-utilization 0.85"
        ]
        ports:
          - containerPort: 8000
        volumeMounts:
        - name: model-volume
          mountPath: /model/meta-llama/Llama-3.2-1B-Instruct
        - name: dshm  # 挂载共享内存到容器
          mountPath: /dev/shm
        # 可选：添加资源限制
        resources:
          limits:
            nvidia.com/nvidia-rtx-3090-24GB: 2
            memory: "32Gi"
            cpu: "8"
          requests:
            nvidia.com/nvidia-rtx-3090-24GB: 2
            memory: "16Gi"
            cpu: "4"
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-server
spec:
  selector:
    app.kubernetes.io/name: vllm
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
```


多机多卡

```bash

apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
    spec:
      # 直接指定节点名称，强制调度到该节点
      nodeName: system-test-100-16-gpu-3090
      # 添加共享内存卷配置
      volumes:
      - name: model-volume
        hostPath:
          path: /root/wyq/model
          type: Directory
      - name: dshm  # 共享内存卷
        emptyDir:
          medium: Memory
          sizeLimit: 10Gi  # 10G共享内存
      containers:
      - name: vllm
        image: registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124
        command: ["/bin/sh", "-c"]
        args: [
          "sleep 36000"
        ]
        ports:
          - containerPort: 8000
        volumeMounts:
        - name: model-volume
          mountPath: /model/meta-llama/Llama-3.2-1B-Instruct
        - name: dshm  # 挂载共享内存到容器
          mountPath: /dev/shm
        # 可选：添加资源限制
        resources:
          limits:
            nvidia.com/nvidia-rtx-3090-24GB: 2
          requests:
            nvidia.com/nvidia-rtx-3090-24GB: 2
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-server
spec:
  selector:
    app.kubernetes.io/name: vllm
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
```

run.sh

![run.sh](./run.sh)

```bash
#!/bin/bash
#
# Helper script to manually start or join a Ray cluster for online serving of vLLM models.
# This script is first executed on the head node, and then on each worker node with the IP address
# of the head node.
#
# Subcommands:
#   leader: Launches a Ray head node and blocks until the cluster reaches the expected size (head + workers).
#   worker: Starts a worker node that connects to an existing Ray head node.
#
# Example usage:
# On the head node machine, start the Ray head node process and run a vLLM server.
#   ./multi-node-serving.sh leader --ray_port=6379 --ray_cluster_size=<SIZE> [<extra ray args>]  && \
#   python3 -m vllm.entrypoints.openai.api_server --port 8080 --model meta-llama/Meta-Llama-3.1-405B-Instruct --tensor-parallel-size 8 --pipeline_parallel_size 2
# 
# On each worker node, start the Ray worker node process.
#   ./multi-node-serving.sh worker --ray_address=<HEAD_NODE_IP> --ray_port=6379 [<extra ray args>]
#
# About Ray:
# Ray is an open-source distributed execution framework that simplifies
# distributed computing. Learn more:
# https://ray.io/


subcommand=$1  # Either "leader" or "worker".
shift          # Remove the subcommand from the argument list.

ray_port=6379              # Port used by the Ray head node.
ray_init_timeout=300       # Seconds to wait before timing out.
declare -a start_params    # Parameters forwarded to the underlying 'ray start' command.

# Handle the worker subcommand.
case "$subcommand" in
  worker)
    ray_address=""
    while [ $# -gt 0 ]; do
      case "$1" in
        --ray_address=*)
          ray_address="${1#*=}"
          ;;
        --ray_port=*)
          ray_port="${1#*=}"
          ;;
        --ray_init_timeout=*)
          ray_init_timeout="${1#*=}"
          ;;
        *)
          start_params+=("$1")
      esac
      shift
    done

    if [ -z "$ray_address" ]; then
      echo "Error: Missing argument --ray_address"
      exit 1
    fi

    # Retry until the worker node connects to the head node or the timeout expires.
    for (( i=0; i < $ray_init_timeout; i+=5 )); do
      ray start --address=$ray_address:$ray_port --block "${start_params[@]}"
      if [ $? -eq 0 ]; then
        echo "Worker: Ray runtime started with head address $ray_address:$ray_port"
        exit 0
      fi
      echo "Waiting until the ray worker is active..."
      sleep 5s;
    done
    echo "Ray worker starts timeout, head address: $ray_address:$ray_port"
    exit 1
    ;;

  # Handle the leader subcommand.
  leader)
    ray_cluster_size=""
    while [ $# -gt 0 ]; do
          case "$1" in
            --ray_port=*)
              ray_port="${1#*=}"
              ;;
            --ray_cluster_size=*)
              ray_cluster_size="${1#*=}"
              ;;
            --ray_init_timeout=*)
              ray_init_timeout="${1#*=}"
              ;;
            *)
              start_params+=("$1")
          esac
          shift
    done

    if [ -z "$ray_cluster_size" ]; then
      echo "Error: Missing argument --ray_cluster_size"
      exit 1
    fi

    # Start the Ray head node.
    ray start --head --port=$ray_port "${start_params[@]}"

    # Poll Ray until every worker node is active.
    for (( i=0; i < $ray_init_timeout; i+=5 )); do
        active_nodes=`python3 -c 'import ray; ray.init(); print(sum(node["Alive"] for node in ray.nodes()))'`
        if [ $active_nodes -eq $ray_cluster_size ]; then
          echo "All ray workers are active and the ray cluster is initialized successfully."
          exit 0
        fi
        echo "Wait for all ray workers to be active. $active_nodes/$ray_cluster_size is active"
        sleep 5s;
    done

    echo "Waiting for all ray workers to be active timed out."
    exit 1
    ;;

  *)
    echo "unknown subcommand: $subcommand"
    exit 1
    ;;
esac
```

vllm 服务验证

vllm-server-6fbcbcfc47-b66nj   1/1     Running                    0          36s   10.244.167.235   system-test-100-16-gpu-3090   <none>           <none>
vllm-server-6fbcbcfc47-nbsm5   1/1     Running                    0          35s   10.244.167.202   system-test-100-16-gpu-3090   <none>           <none>

```bash
leader:

bash run.sh leader --port=6379 --ray_cluster_size=2
python3 -m vllm.entrypoints.openai.api_server --port 8080 --model /model/meta-llama/Llama-3.2-1B-Instruct/ --tensor-parallel-size 2 --pipeline_parallel_size 2


worker:

bash run.sh worker --ray_port=6379 --ray_address=10.244.167.194

```

依赖ray框架做任务分发。
leader节点指定通信的端口和参与的leader和worker的数量
worker节点指定和leader节点通行的端口和leader的ip地址。

最后在leader节点下发训练任务

```bash
curl http://10.244.167.194:8080/v1/models
{"object":"list","data":[{"id":"/model/meta-llama/Llama-3.2-1B-Instruct/","object":"model","created":1754898219,"owned_by":"vllm","root":"/model/meta-llama/Llama-3.2-1B-Instruct/","parent":null,"max_model_len":131072,"permission":[{"id":"modelperm-ad1678dda5574a51870435b3c66b9cc4","object":"model_permission","created":1754898219,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}
```

可以通过curl pod ip进行接口访问。



sglang 功能验证


```bash
leader:

export LWS_GROUP_SIZE=2
export LWS_LEADER_ADDRESS=10.244.167.218
export LWS_WORKER_INDEX=0
python3 -m sglang.launch_server \
    --model-path /model/meta-llama/Llama-3.2-1B-Instruct/ \
    --served-model-name Llama-3.2-1B-Instruct \
    --tp 2  \
    --dist-init-addr $LWS_LEADER_ADDRESS:20000 \
    --nnodes $LWS_GROUP_SIZE \
    --node-rank $LWS_WORKER_INDEX \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 18000 

worker:

export LWS_GROUP_SIZE=2
export LWS_LEADER_ADDRESS=10.244.167.218
export LWS_WORKER_INDEX=1
python3 -m sglang.launch_server \
    --model-path /model/meta-llama/Llama-3.2-1B-Instruct/ \
    --served-model-name Llama-3.2-1B-Instruct \
    --tp 2  \
    --dist-init-addr $LWS_LEADER_ADDRESS:20000 \
    --nnodes $LWS_GROUP_SIZE \
    --node-rank $LWS_WORKER_INDEX \
    --trust-remote-code 
```


```bash
[root@system-test-100-16-gpu-3090 ~]# curl http://10.244.167.218:18000/v1/models
{"object":"list","data":[{"id":"Llama-3.2-1B-Instruct","object":"model","created":1754899733,"owned_by":"sglang","root":"Llama-3.2-1B-Instruct"}]}
```

leader  节点接受请求



命令不一样：可以通过脚本进行封装成一个命令

leader需要在worker前启动，可以通过构建StatefulSet来让leader命令先执行

同时可以解决因镜像不存在的场景


通过验证statefulset是可以实现分布式推理的。
