
# 分布式推理详细设计

## 方案背景

在当前类脑云平台增加分布式推理的功能。

待解决的问题：

1. leader pod的启动需要在worker pod的启动之前。
2. 针对分布式推理需要的一些环境变量统一注入到每个pod中。

## 功能验证

### lws

LWS是Kubernetes社区下特别兴趣小组（SIG）提出的一种新的工作负载类型。它与Deployment、StatefulSet等Kubernetes原生工作负载的核心区别在于，LeaderWorkerSet将一组Pod而不是一个Pod看作一个副本。副本的一次扩容通常意味着一次性扩容多个Pod，并且一个副本内的多个Pod存在Leader Pod和Worker Pod的主从关系。这种工作负载类型有利于更好地支撑AI/ML的多机分布式推理任务。

### statefulset

#### vllm 功能验证

验证镜像；registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124

验证yaml文件： [statefulset-vllm.yaml](./statefulset-vllm.yaml)

run.sh

![run.sh](./run.sh)

验证命令：

```bash
leader:

bash run.sh leader --port=6379 --ray_cluster_size=2
python3 -m vllm.entrypoints.openai.api_server --port 8080 --model /model/meta-llama/Llama-3.2-1B-Instruct/ --tensor-parallel-size 2 --pipeline_parallel_size 2


worker:

bash run.sh worker --ray_port=6379 --ray_address=10.244.167.194

```

依赖ray框架做任务分发。
leader节点指定通信的端口和参与的leader和worker的数量
worker节点指定和leader节点通行的端口和leader的ip地址。

最后在leader节点下发训练任务

```bash
curl http://10.244.167.194:8080/v1/models
{"object":"list","data":[{"id":"/model/meta-llama/Llama-3.2-1B-Instruct/","object":"model","created":1754898219,"owned_by":"vllm","root":"/model/meta-llama/Llama-3.2-1B-Instruct/","parent":null,"max_model_len":131072,"permission":[{"id":"modelperm-ad1678dda5574a51870435b3c66b9cc4","object":"model_permission","created":1754898219,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}
```

可以通过curl pod ip进行接口访问。

#### sglang 功能验证

验证镜像：registry.cnbita.com:5000/lgy/sglang/sglang:v0.4.4.post1-cu124
验证yaml文件：[statefulset-sgland.yaml](./statefulset-sgland.yaml)

验证命令如下：

```bash
leader:

export LWS_GROUP_SIZE=2
export LWS_LEADER_ADDRESS=10.244.167.218
export LWS_WORKER_INDEX=0
python3 -m sglang.launch_server \
    --model-path /model/meta-llama/Llama-3.2-1B-Instruct/ \
    --served-model-name Llama-3.2-1B-Instruct \
    --tp 2  \
    --dist-init-addr $LWS_LEADER_ADDRESS:20000 \
    --nnodes $LWS_GROUP_SIZE \
    --node-rank $LWS_WORKER_INDEX \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 18000 

worker:

export LWS_GROUP_SIZE=2
export LWS_LEADER_ADDRESS=10.244.167.218
export LWS_WORKER_INDEX=1
python3 -m sglang.launch_server \
    --model-path /model/meta-llama/Llama-3.2-1B-Instruct/ \
    --served-model-name Llama-3.2-1B-Instruct \
    --tp 2  \
    --dist-init-addr $LWS_LEADER_ADDRESS:20000 \
    --nnodes $LWS_GROUP_SIZE \
    --node-rank $LWS_WORKER_INDEX \
    --trust-remote-code 
```

```bash
[root@system-test-100-16-gpu-3090 ~]# curl http://10.244.167.218:18000/v1/models
{"object":"list","data":[{"id":"Llama-3.2-1B-Instruct","object":"model","created":1754899733,"owned_by":"sglang","root":"Llama-3.2-1B-Instruct"}]}
```

### keda 控制statefulset进行扩缩

验证方式： 新建控制器，通过创建scaledobject来控制控制statefulset的数量

验证项目: [实验项目](./distritueInfer/)

验证结果： 可以正常扩缩容

## 详细设计

lws 针对leader和worker 任务会分别建立一个statefulset，如果是多副本的情况下，worker会有多个statefulset生成。

优点： 减少开发代码量。
缺点： 引入新的推理组件，增加维护难度，出现问题的定位较麻烦，

通过statefulset的方式进行实现，将worker和leader 写入同一个statefulset中，多副本可以通过新建多个statefulset的方式实现。

命令不一样：可以通过脚本进行封装成一个命令
leader需要在worker前启动，可以通过构建StatefulSet来让leader命令先执行

缺点：增加一些代码开发量
优点：不引入新的组件，减少运维难度。

**目前推荐使用statefulset的方式进行实现，因为lws的代码逻辑不是很复杂，在当前的控制器逻辑中完全可以自己实现，增加的代码量也不多。**

### CR 定义

```bash
# The InferenceService
apiVersion: system.hero.ai/v1alpha1
kind: InferenceService
metadata:
  name: simple-inference-service
  namespace: default
  labels:
    app.kubernetes.io/name: inferenceservice
    app.kubernetes.io/managed-by: kustomize
spec:
  deploymentMode: DistributeInference
  scheduler: 
    queue: "a15155090721337344296789" #资源池
  replicas: 1   # statefulset的数量
  servers:
    leader:
      replicas: 1
      template:
        metadata:
          annotations:  # 适配roce网卡资源
            k8s.v1.cni.cncf.io/networks: macvlan-conf1
        spec:
          containers:
          - name: inference-server
            image: registry.cnbita.com:5000/heros-dev/python:3.9-slim
            args: 
            - |
              “”
            ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            env:
            - name: NCCL_IB_GID_INDEX   
              value: "5"
            - name: NCCL_SOCKET_IFNAME
              value: "eth0"
            - name: NCCL_IB_TC
              value: "128"
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "200m"
                memory: "256Mi"
    worker:
      replicas: 1
      template:
        metadata:
          annotations:
            k8s.v1.cni.cncf.io/networks: macvlan-conf1
        spec:
          containers:
          - name: external-image-test
            image: registry.cn-hangzhou.aliyuncs.com/sea-wyq/python:3.9-amd64
            imagePullPolicy: Always
            command: ["/bin/bash", "-c", "sleep 3600"]
            env:
            - name: USER_REGISTRY_USERNAME
              value: "wyq_ww"
            - name: USER_REGISTRY_PASSWORD
              value: "wyq980315"
            - name: USER_REGISTRY_SERVER
              value: "registry.cn-hangzhou.aliyuncs.com"
            - name: NCCL_IB_GID_INDEX   
              value: "5"
            - name: NCCL_SOCKET_IFNAME
              value: "eth0"
            - name: NCCL_IB_TC
              value: "128"
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "200m"
                memory: "256Mi"
```

### 功能描述

（1）弹性伸缩  （ keda + scale ）
（2）serverless （keda 在从0到1的过程中，是否能正常谈起pod数量到正常的pod的数量 ）
（3）多副本
（4）集成roce网卡资源 （注解 + 环境变量）
（5）提供大模型模版
（6）service mesh （不需要支持）

## 波及分析

## 参考文档

<https://lws.sigs.k8s.io/docs/examples/vllm/>