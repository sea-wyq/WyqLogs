DeepSeek-R1: åœ¨å¼€å‘ç¯å¢ƒéƒ¨ç½²å¹¶å¯¹è¯
===================================

DeepSeek-R1 æ˜¯ä¸€æ¬¾ç”±æ·±åº¦æ±‚ç´¢æ¨å‡ºçš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‰çš„å†·å¯åŠ¨æ•°æ®è®­ç»ƒï¼Œæœ‰æ•ˆè§£å†³äº†é‡å¤ç”Ÿæˆã€å¯è¯»æ€§å·®åŠå¤šè¯­è¨€æ··æ‚ç­‰é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç ä¸é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæ€§èƒ½å¯¹æ ‡ OpenAI-o1ã€‚å…¶åˆ›æ–°è®­ç»ƒæ¡†æ¶çªç ´äº†ä¼ ç»Ÿ RL è®­ç»ƒçš„å±€é™æ€§ï¼Œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚

æœ¬æ–‡ä»¥ [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) ä¸ºä¾‹ï¼Œåœ¨ JupyterLab å¼€å‘ç¯å¢ƒä¸­éƒ¨ç½²æ¨¡å‹å¹¶è¿›è¡Œå¯¹è¯ã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š

- [DeepSeek-R1: åœ¨å¼€å‘ç¯å¢ƒéƒ¨ç½²å¹¶å¯¹è¯](#deepseek-r1-åœ¨å¼€å‘ç¯å¢ƒéƒ¨ç½²å¹¶å¯¹è¯)
  - [1. ä¸‹è½½æ¨¡å‹æ–‡ä»¶](#1-ä¸‹è½½æ¨¡å‹æ–‡ä»¶)
  - [2. ä¸Šä¼ æ¨¡å‹æ–‡ä»¶](#2-ä¸Šä¼ æ¨¡å‹æ–‡ä»¶)
  - [3. åˆ›å»ºæ¨¡å‹](#3-åˆ›å»ºæ¨¡å‹)
  - [4. åˆ›å»º JupyterLab å¼€å‘ç¯å¢ƒ](#4-åˆ›å»º-jupyterlab-å¼€å‘ç¯å¢ƒ)
  - [5. å‡†å¤‡æ¨ç†è„šæœ¬](#5-å‡†å¤‡æ¨ç†è„šæœ¬)
  - [6. éƒ¨ç½²æ¨¡å‹å¹¶å¯¹è¯](#6-éƒ¨ç½²æ¨¡å‹å¹¶å¯¹è¯)

## 1. ä¸‹è½½æ¨¡å‹æ–‡ä»¶

æœ¬æ¬¡éƒ¨ç½²çš„æ¨¡å‹æ˜¯ [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤å°†æ¨¡å‹æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼ˆçº¦14.1GBï¼‰ã€‚

```shell
pip install -U huggingface_hub
export HF_ENDPOINT=https://hf-mirror.com 
huggingface-cli download --resume-download deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --local-dir ./
```

ä¹Ÿå¯é€‰æ‹©ä¸‹è½½ DeepSeek-R1 Models æˆ– DeepSeek-R1-Distill Models.

<div align="center">

|    **Model**     | **#Total Params** | **#Activated Params** | **Context Length** |                             **Download**                             |
| :--------------: | :---------------: | :-------------------: | :----------------: | :------------------------------------------------------------------: |
| DeepSeek-R1-Zero |       671B        |          37B          |        128K        | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) |
|   DeepSeek-R1    |       671B        |          37B          |        128K        |   [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)    |

</div>

<div align="center">

|           **Model**           |                                   **Base Model**                                   |                                   **Download**                                    |
| :---------------------------: | :--------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------: |
| DeepSeek-R1-Distill-Qwen-1.5B |         [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B)         | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) |
|  DeepSeek-R1-Distill-Qwen-7B  |           [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B)           |  [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)  |
| DeepSeek-R1-Distill-Llama-8B  |           [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)           | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)  |
| DeepSeek-R1-Distill-Qwen-14B  |               [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B)               | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)  |
| DeepSeek-R1-Distill-Qwen-32B  |               [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B)               | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)  |
| DeepSeek-R1-Distill-Llama-70B | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ğŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) |

</div>

## 2. ä¸Šä¼ æ¨¡å‹æ–‡ä»¶

è¯·åœ¨æ–‡ä»¶å­˜å‚¨ä¸­åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿï¼ˆå·²å­˜åœ¨å¯ä¸åˆ›å»ºï¼‰ï¼Œå¹¶ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·å°†ä¸‹è½½å¥½çš„æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸Šä¼ åˆ°å¯¹åº”çš„æ–‡ä»¶ç³»ç»Ÿä¸­ã€‚ä¸‹æ–¹ç¤ºä¾‹ä¸­ï¼Œæ–‡ä»¶ç³»ç»Ÿåç§°ä¸º DeepSeek.

![åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ-2025-03-04.jpg)

![å‘½ä»¤è¡Œå·¥å…·ä¸Šä¼ -2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/å‘½ä»¤è¡Œå·¥å…·ä¸Šä¼ -2025-03-04.jpg)

![æ¨¡å‹æ–‡ä»¶ä¸Šä¼ ç»“æœ-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/æ¨¡å‹æ–‡ä»¶ä¸Šä¼ ç»“æœ-2025-03-04.jpg)

## 3. åˆ›å»ºæ¨¡å‹

ä¸ºäº†ä¾¿äºåç»­åå¤ä½¿ç”¨ï¼Œå°†æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ¨¡å‹æ–‡ä»¶ï¼Œåœ¨å¹³å°çš„ã€Œæ¨¡å‹ã€æ¿å—ä¸­åˆ›å»ºå¯¹åº”çš„ DeepSeek-R1 æ¨¡å‹ã€‚

ç‚¹å‡»å·¦è¾¹æ æ¨¡å‹æŒ‰é’®ï¼Œç„¶åç‚¹å‡»åˆ›å»ºæ¨¡å‹æŒ‰é’®ï¼Œé€šè¿‡ä¸Šé¢ä¸Šä¼ çš„æ¨¡å‹æ–‡ä»¶æ¥åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œåˆ›å»ºæ¨¡å‹åå¦‚ä¸‹æ‰€ç¤ºã€‚

![åˆ›å»ºæ¨¡å‹-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºæ¨¡å‹-2025-03-04.jpg)

![æ¨¡å‹åˆ—è¡¨-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/æ¨¡å‹åˆ—è¡¨-2025-03-04.jpg)

## 4. åˆ›å»º JupyterLab å¼€å‘ç¯å¢ƒ

åœ¨ã€Œæ¨¡å‹å¼€å‘å’Œè®­ç»ƒã€ä¸­ï¼Œåˆ›å»ºæ–°çš„å¼€å‘ç¯å¢ƒã€‚

1. åœ¨ã€Œå­˜å‚¨æŒ‚è½½ã€ä¸­ç‚¹å‡»æ¨¡å‹ï¼Œæ·»åŠ ä¸Šé¢åˆ›å»ºçš„æ¨¡å‹ï¼›
2. é€‰æ‹©æ”¯æŒ JupyterLab æˆ– SSH è®¿é—®çš„é•œåƒï¼›
3. é€‰æ‹© 1 GPU å¥—é¤èµ„æºï¼›

![åˆ›å»ºå¼€å‘ç¯å¢ƒ1-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºå¼€å‘ç¯å¢ƒ1-2025-03-04.jpg)
![åˆ›å»ºå¼€å‘ç¯å¢ƒ2-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºå¼€å‘ç¯å¢ƒ2-2025-03-04.jpg)
![åˆ›å»ºå¼€å‘ç¯å¢ƒ3-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºå¼€å‘ç¯å¢ƒ3-2025-03-04.jpg)

ç‚¹å‡»ç¡®è®¤åï¼Œç­‰å¾…ä»»åŠ¡è¿›å…¥è¿è¡ŒçŠ¶æ€ã€‚

![åˆ›å»ºå¼€å‘ç¯å¢ƒ4-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/åˆ›å»ºå¼€å‘ç¯å¢ƒ4-2025-03-04.jpg)

## 5. å‡†å¤‡æ¨ç†è„šæœ¬

æ‰“å¼€ JupyterLab åï¼ŒæŒ‚è½½çš„æ¨¡å‹æ–‡ä»¶åœ¨ `/model/<æ¨¡å‹åç§°>` ç›®å½•ä¸‹ï¼Œæ¯”å¦‚ä¸Šé¢æŒ‚è½½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯ `/model/DeepSeek-R1`.

æˆ‘ä»¬å°†æ¨ç†è„šæœ¬æ”¾åœ¨æ¨¡å‹æ–‡ä»¶ç›®å½•ä¸‹ï¼Œç¼–è¾‘æ–‡ä»¶ `/model/DeepSeek-R1/infer.py` å†™å…¥ä¸‹é¢çš„ä»£ç ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import unicodedata
from typing import List

@torch.inference_mode()
def generate(
    model: AutoModelForCausalLM,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    max_new_tokens: int,
    temperature: float = 1.0
) -> List[int]:
    """
    Generate response from the model with attention_mask provided.
    """
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,  # æä¾›æ˜¾å¼ attention mask
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        eos_token_id=model.config.eos_token_id,
        pad_token_id=model.config.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
    )
    return outputs[0].tolist()

def clean_input(user_input):
    """
    æ¸…ç†ç”¨æˆ·è¾“å…¥ï¼Œå»é™¤ä¸å¯è§å­—ç¬¦å’Œå¤šä½™çš„ç©ºæ ¼ã€‚
    """
    user_input = "".join(c for c in user_input if not unicodedata.category(c).startswith("C"))  # ç§»é™¤æ§åˆ¶å­—ç¬¦
    return user_input.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼

def clean_message_content(content):
    """
    æ¸…ç†æ¶ˆæ¯å†…å®¹ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼å¹¶è¿‡æ»¤éæ³•è¾“å…¥
    """
    if not content or not isinstance(content, str):
        return ""
    return content.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼

def build_prompt(messages, max_history=3):
    """
    Build prompt for the model, limiting the history to the most recent messages.
    """
    template = "The following is a conversation with an AI assistant. The assistant is helpful, knowledgeable, and polite:\n"
    for msg in messages[-max_history:]:
        content = clean_message_content(msg["content"])
        if not content:  # è·³è¿‡ç©ºå†…å®¹
            continue
        template += f"{msg['role'].capitalize()}: {content}\n"
    template += "Assistant: "
    return template.strip()  # ç¡®ä¿è¿”å›å€¼æ˜¯å­—ç¬¦ä¸²

if __name__ == "__main__":
    print("Initializing DeepSeek-R1 Service...")

    # Configuration
    ckpt_path = "./DeepSeek-R1-Distill-Qwen-7B"  # æ¨¡å‹æ‰€åœ¨çš„ç›®å½•
    config_path = "./DeepSeek-R1-Distill-Qwen-7B/config.json"  # é…ç½®æ–‡ä»¶è·¯å¾„

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)
    model = AutoModelForCausalLM.from_pretrained(
        ckpt_path,
        torch_dtype=torch.bfloat16,
    ).cuda()

    # Interactive session
    messages = []  # To maintain context
    while True:
        user_input = input("You: ").strip()  # å»é™¤é¦–å°¾ç©ºæ ¼
        user_input = clean_input(user_input)  # æ¸…ç†ä¸å¯è§å­—ç¬¦
        if not user_input or len(user_input.strip()) == 0:  # æ£€æŸ¥æ— æ•ˆè¾“å…¥
            print("Invalid input. Please type something meaningful!")
            continue

        if user_input.lower() in ["exit", "quit"]:
            print("Exiting conversation. Goodbye!")
            break

        # Append user input to context
        messages.append({"role": "user", "content": user_input})

        # Limit conversation history
        messages = messages[-10:]  # åªä¿ç•™æœ€è¿‘ 10 æ¡å¯¹è¯

        # Build prompt and tokenize
        prompt = build_prompt(messages)
        if not isinstance(prompt, str) or len(prompt.strip()) == 0:  # ç¡®ä¿ prompt éç©º
            print("Error: Prompt is empty or invalid. Skipping this turn.")
            continue

        tokenized_prompt = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
        input_ids = tokenized_prompt["input_ids"].to("cuda")
        attention_mask = tokenized_prompt["attention_mask"].to("cuda")

        # Generate response
        max_new_tokens = 150
        temperature = 0.7

        completion_tokens = generate(model, input_ids, attention_mask, max_new_tokens, temperature)
        completion = tokenizer.decode(
            completion_tokens[len(input_ids[0]):],  # ä»è¾“å…¥é•¿åº¦æˆªå–ç”Ÿæˆéƒ¨åˆ†
            skip_special_tokens=True
        ).split("User:")[0].strip()

        print(f"Assistant: {completion}")

        # Append assistant response to context
        messages.append({"role": "assistant", "content": completion})
```

## 6. éƒ¨ç½²æ¨¡å‹å¹¶å¯¹è¯

åœ¨ JupyterLab ä¸­æ‰“å¼€ Terminal å¹¶è¿›å…¥ `/model/DeepSeek-R1` ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ `python3 infer.py` å³å¯å’Œ DeepSeek-R1 è¿›è¡Œå¯¹è¯ã€‚

> æ³¨ï¼šå› ä¸Šè¿°æ‰€é€‰çš„é•œåƒæ²¡æœ‰å®‰è£… `transformers` python åŒ…ï¼Œéœ€è¦é€šè¿‡ `pip install transformers` è¿›è¡Œå®‰è£…ã€‚

![è¿è¡Œè„šæœ¬å¹¶å¯¹è¯-2025-03-04](https://fourt-wyq.oss-cn-shanghai.aliyuncs.com/images/è¿è¡Œè„šæœ¬å¹¶å¯¹è¯-2025-03-04.jpg)
